[
["index.html", "Biostatistics using R Preface", " Biostatistics using R Dewey Brooke 2018-06-29 Preface This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["1-intro.html", "1 Introduction ", " 1 Introduction "],
["reading-data-files-into-r.html", "Reading Data Files into R", " Reading Data Files into R The first step in every analysis requires data to be read into the environment, and learning how to do this is the first hurdle a person needs to overcome to begin learning to use R. Data can exist in many different formats, either as the generic universal types (e.g. csv, tsv, .json, etc) or software specific types (e.g. .xlsx, `` ) In this chapter, we will first discuss how to read data using functions in Base-R (when possible), and then we will discuss alternative packages, such as the multitude of packages in the Tidyverse, and highlight their advantages over Base-R functions. 1.0.1 Generic Formats 1.0.1.1 CSV- Comma Separated Values The fields are separated by a comma , and are typically used for loading into spreadsheets. For example: csv_example_path &lt;- &quot;data/ASCII-comma/FEV.DAT.txt&quot; readLines(csv_example_path)[1:8] # reads each line of the file [1] &quot;&#39;Id&#39;,&#39;Age&#39;,&#39;FEV&#39;,&#39;Hgt&#39;,&#39;Sex&#39;,&#39;Smoke&#39;&quot; [2] &quot;301,9,1.708,57,0,0&quot; [3] &quot;451,8,1.724,67.5,0,0&quot; [4] &quot;501,7,1.72,54.5,0,0&quot; [5] &quot;642,9,1.558,53,1,0&quot; [6] &quot;901,9,1.895,57,1,0&quot; [7] &quot;1701,8,2.336,61,0,0&quot; [8] &quot;1752,6,1.919,58,0,0&quot; # Note: readLines(csv_example_path) is the same as # readLines(&quot;data/ASCII-comma/FEV.DAT.txt&quot;) In Base-R, CSV data can be read using the read.csv() function. The read.csv2() function is used in countries that use a comma as a decimal point and a semicolon as a field separator. csv_example &lt;- read.csv(csv_example_path) head(csv_example) X.Id. X.Age. X.FEV. X.Hgt. X.Sex. X.Smoke. 1 301 9 1.708 57.0 0 0 2 451 8 1.724 67.5 0 0 3 501 7 1.720 54.5 0 0 4 642 9 1.558 53.0 1 0 5 901 9 1.895 57.0 1 0 6 1701 8 2.336 61.0 0 0 1.0.1.2 TSV- Tab Separated Values The fields are separated by a tabulation or and are saved as .txt files. However, not all .txt files contain tab separated values. For example: tsv_example_path &lt;- &quot;data/ASCII-tab/FEV.DAT.txt&quot; readLines(tsv_example_path)[1:8] [1] &quot;&#39;Id&#39;\\t&#39;Age&#39;\\t&#39;FEV&#39;\\t&#39;Hgt&#39;\\t&#39;Sex&#39;\\t&#39;Smoke&#39;&quot; [2] &quot;301\\t9\\t1.708\\t57\\t0\\t0&quot; [3] &quot;451\\t8\\t1.724\\t67.5\\t0\\t0&quot; [4] &quot;501\\t7\\t1.72\\t54.5\\t0\\t0&quot; [5] &quot;642\\t9\\t1.558\\t53\\t1\\t0&quot; [6] &quot;901\\t9\\t1.895\\t57\\t1\\t0&quot; [7] &quot;1701\\t8\\t2.336\\t61\\t0\\t0&quot; [8] &quot;1752\\t6\\t1.919\\t58\\t0\\t0&quot; tsv_example &lt;- read.delim(&quot;data/ASCII-tab/FEV.DAT.txt&quot;) head(tsv_example) X.Id. X.Age. X.FEV. X.Hgt. X.Sex. X.Smoke. 1 301 9 1.708 57.0 0 0 2 451 8 1.724 67.5 0 0 3 501 7 1.720 54.5 0 0 4 642 9 1.558 53.0 1 0 5 901 9 1.895 57.0 1 0 6 1701 8 2.336 61.0 0 0 1.0.2 Excel library(readxl) 1.0.3 Software Specific Formats R is increasingly recognized as the gold standard for statistical computations, yet some of your future collaborates will exclusively use Commercial Software (SAS, SPSS, Matlab, and Stata) for their statistical computations. Although these individuals are limited by the types of files they can read or write, the haven R-package can both read and write any of these file formats. library(haven) 1.0.3.1 SAS(.sas7bdat), SPSS(.sav,.por, .xpt), Stata (.dta) sas &lt;- read_sas(&quot;data/SAS/FEV.sas7bdat&quot;) head(sas) # A tibble: 6 x 6 ID AGE FEV HGT SEX SMOKE &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 301 9 1.71 57 0 0 2 451 8 1.72 67.5 0 0 3 501 7 1.72 54.5 0 0 4 642 9 1.56 53 1 0 5 901 9 1.90 57 1 0 6 1701 8 2.34 61 0 0 spss &lt;- read_spss(&quot;data/SPSS/FEV.DAT.sav&quot;) head(spss) # A tibble: 6 x 6 Id Age FEV Hgt Sex Smoke &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 301 9 1.71 57 0 0 2 451 8 1.72 67.5 0 0 3 501 7 1.72 54.5 0 0 4 642 9 1.56 53 1 0 5 901 9 1.90 57 1 0 6 1701 8 2.34 61 0 0 stata &lt;- read_stata(&quot;data/Stata/FEV.DAT.dta&quot;) head(stata) # A tibble: 6 x 6 Id Age fev Hgt Sex Smoke &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 301 9 1.71 57 0 0 2 451 8 1.72 67.5 0 0 3 501 7 1.72 54.5 0 0 4 642 9 1.56 53 1 0 5 901 9 1.90 57 1 0 6 1701 8 2.34 61 0 0 The foreign package included in Base-R can also be used to Reading and writing data stored by some versions of ‘Epi Info’, ‘Minitab’, ‘S’, ‘SAS’, ‘SPSS’, ‘Stata’, ‘Systat’, ‘Weka’,and for reading and writing some ‘dBase’ files. 1.0.3.1.0.1 RDS rds_example &lt;- readRDS(&quot;data/RDS/BETACAR.DAT.rds&quot;) head(rds_example) # A tibble: 6 x 8 `&#39;Prepar&#39;` `&#39;Id&#39;` `&#39;Base1lvl&#39;` `&#39;Base2lvl&#39;` &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 71 298 116 2 1 73 124 146 3 1 80 176 200 4 1 83 116 180 5 1 90 152 142 6 1 92 106 106 # ... with 4 more variables: `&#39;Wk6lvl&#39;` &lt;int&gt;, # `&#39;Wk8lvl&#39;` &lt;int&gt;, `&#39;Wk10lvl&#39;` &lt;int&gt;, # `&#39;Wk12lvl&#39;` &lt;int&gt; 1.0.3.1.0.2 rdata The .rdata format is R’s specific format. Instead of using a read.{something} function, .rdata is read into the environment using load(filename.rdata) and retains the original name it had when it was last saved. load(&quot;data/R/BETACAR.DAT.rdata&quot;) #named betacar when it was last saved head(betacar) Prepar Id Base1lvl Base2lvl Wk6lvl Wk8lvl Wk10lvl 1 1 71 298 116 174 178 218 2 1 73 124 146 294 278 244 3 1 80 176 200 276 286 308 4 1 83 116 180 164 238 308 5 1 90 152 142 290 300 270 6 1 92 106 106 246 206 304 Wk12lvl 1 190 2 262 3 334 4 226 5 268 6 356 "],
["2-descriptive-statistics.html", "2 Descriptive Statistics", " 2 Descriptive Statistics "],
["2-1-measures-of-location-using-base-r.html", "2.1 Measures of Location using Base R", " 2.1 Measures of Location using Base R Determining the correct method for measuring the central tendancy of a vector depends on the relationship between the numbers within the vector. Numbers that can be summed in a linear sequence are best represented using the arithmic mean. If you’re measuring units that add up as reciprocals in a sequence (such as speed or distance / time over a constant distance, capacitance in series, resistance in parallel), then a harmonic mean will give you a meaningful average. For example, the harmonic mean of capacitors in series represents the capacitance that a single capacitor would have if only one capacitor was used instead of the set of capacitors in series. If you’re measuring units that multiply in a sequence (such as growth rates or percentages), then a geometric mean will give you a meaningful average. For example, the geometric mean of a sequence of different annual interest rates over 10 years represents an interest rate that, if applied constantly for ten years, would produce the same amount growth in principal as the sequence of different annual interest rates over ten years did. Does an arithmetic mean of interest rates have any significance? As a number, sure. But as an “average” interest rate it seems less intuitive because the principal it produces at the end of ten years is much larger than the geometric mean. Similarly, the harmonic mean of interest rates produces a smaller principal, and so is less intuitive. Now consider areas and volumes as a test of understanding. What mean should we use to report the “average” area or volume in a sequence of areas or volumes? Area is measured in units of length squared. Volume is measured in units of length cubed. In a sequence of areas or volumes, we could either add them up linearly and divide or multiply them and take the roots — which is correct? It depends on what we’re measuring. If these areas or volumes are dependent upon each other (e.g., the size of the same microbe at different times), then a geometric mean probably makes more sense. If these areas or volumes are independent of each other (e.g., the size of a house or pool), then an arithmetic mean probably makes more sense. But whatever you decide, when in doubt report that decision. There is nothing worse for a reader than to see an “average” and not know how it was calculated! - Michael F. Martin,Quora Answer 2.1.1 The Arithmetic Mean The arithmetic mean is the sum of all the observations divided by the number of observations. It is written in statistical terms as \\[\\overline{x} = \\frac{1}{n}\\sum^n_{i=1}x_i\\] mean(ChickWeight$weight) [1] 121.8 2.1.2 The Median The sample median is: If n is odd \\(\\rightarrow\\) \\(\\Big(\\frac{n+1}{2}\\Big)\\text{th}\\) largest observation If n is even \\(\\rightarrow\\) \\(\\Big(\\frac{n}{2}\\Big)\\text{th}\\) and \\(\\Big(\\frac{n}{2}+1\\Big)\\text{th}\\) largest observations median(ChickWeight$weight) [1] 103 2.1.3 The Mode The mode is the most frequently occurring value among all observations in the sample. Although it is infrequently used, it is very useful for categorical and discrete data. Since there isn’t a built in R-function for mode, we learn how to write a function to return the mode through a few examples. 2.1.3.1 Functions 2.1.3.1.1 Base R Example The most simple function begins by assigning the output of function() to some character string (e.g. simple_fun) All statements after the function() are referred as the body of the function. function_name &lt;- function(arg1, arg2,...) { #statements return(&quot;some output&quot;) } function_name() # returns NULL [1] &quot;some output&quot; Use return() to output the result of the function. return_value &lt;- function(x,y) { z=x-y z=x+y return(z) } return_value(4,5) [1] 9 Since our goal is to find the most frequently occurring value in our data-set (ChickWeight), we need to decide the sequence of functions that we need to accomplish this. As you continue to add various R functions to your R tool belt, you will find many possible combinations for the same solution. First, let’s assign the weight column from ChickWeight to x to simplify things. When x is called, the weight column from ChickWeight is returned as a vector. x&lt;-ChickWeight$weight head(x) [1] 42 51 59 64 76 93 We can return the size of x using the length function. 578 length(x) [1] 578 We can reduce x to return only the unique values by using the unique function. We’ll assign it to y so we can use it later. y &lt;- unique(x) length(y) [1] 212 To more easily watch how the functions are working, we will create two data-frames to watch how we are manipulating both x and y. df.x &lt;- data.frame(x) df.y &lt;- data.frame(y) Using the unique values from the x vector we defined as y, we can use the match function to return a vector that replaces each value in x with their position in the y vector (1-212). df.x$position_in_y&lt;-match(x, y) head(df.x, n = 30) x position_in_y 1 42 1 2 51 2 3 59 3 4 64 4 5 76 5 6 93 6 7 106 7 8 125 8 9 149 9 10 171 10 11 199 11 12 205 12 13 40 13 14 49 14 15 58 15 16 72 16 17 84 17 18 103 18 19 122 19 20 138 20 21 162 21 22 187 22 23 209 23 24 215 24 25 43 25 26 39 26 27 55 27 28 67 28 29 84 17 30 99 29 The output from match can then be simplified using the tabulate function df.y$frequency &lt;- tabulate(df.x$position_in_y) head(df.y) y frequency 1 42 15 2 51 8 3 59 5 4 64 5 5 76 3 6 93 4 which.max returns the position of the maximum value. which.max(df.y$frequency) [1] 43 df.y[43,] #df.y[row,column] y frequency 43 41 20 Putting it all together, we can do this in one line. df.y[which.max(tabulate(match(x,y))),] y frequency 43 41 20 y[which.max(tabulate(match(x,y)))] [1] 41 Writing this as a function mode &lt;- function(x){ unique_x &lt;- unique(x) result&lt;-unique_x[which.max(tabulate(match(x,unique_x)))] return(result) } mode(x) [1] 41 2.1.3.1.2 Tidyverse Example As with most problems in R, we can also find a solution using packages from the Tidyverse. We will therefore use this as an opportunity to introduce some of the basic tenants of Tidyverse functions. In the dplyr package, a typical workflow will combine observations into a single data-frame, aggregate them into groups, manipulate values into new columns, and summaries the data-frame into more simple terms. The piping operator %&gt;% allows for this to be done seamlessly by literally pipping the result of one function into arguments of another function. print(&quot;non-piped text&quot;) [1] &quot;non-piped text&quot; library(dplyr) &quot;piped text&quot; %&gt;% print() [1] &quot;piped text&quot; To show how this works, we will start with a simple example where we first want to divided the sum of three and some other number (e.g. 2) by seven. Because of the order of operations, the sum of two and three would need to be placed with parenthesis to indicate it happens before dividing by seven. (4+3)/7 # correct [1] 1 4 + 3 / 7 # incorrect [1] 4.429 The piping operator allows the order of operations be explicated dictated with manipulations of starting value reading from the left to right. # pipes use the (.) as a placeholder 4 %&gt;% + 3 %&gt;% {./7} # removing the { } returns an error [1] 1 Using pipes increases readability of your R-code and it can easily be reused for different starting values. In R Studio, the pipe character can be easily inserted using a keyboard shortcut (Windows:Ctrl+Shift+M, Mac:Cmd+Shift+M). 11 %&gt;% + 3 %&gt;% {./7} [1] 2 Plus, the piped workflow can easily be defined by a function by assigning it to some string with a . in the beginning. op_order &lt;- . %&gt;% +3 %&gt;% {./7} op_order(4) [1] 1 op_order(11) [1] 2 2.1.3.1.2.1 Determining Mode with dplyr Using the ChickWeight data-set as before, we start by outlining the order of operations. Group the data by weights group_by() Tally the number of members within each group and sort by frequency. tally() Select the row with the largest n. slice() Return the corresponding weight. .$weight ChickWeight %&gt;% group_by(weight) %&gt;% tally(sort = TRUE) %&gt;% slice(1) %&gt;% .$weight [1] 41 As before, this workflow can be written as a function by placing . between the assignment operator &lt;- and piping operator %&gt;%. mode_cw&lt;-. %&gt;% group_by(weight) %&gt;% tally(sort = TRUE) %&gt;% slice(1) %&gt;% .$weight mode_cw(ChickWeight) [1] 41 However, this function will only work on the ChickWeight data-set. mode_cw(mtcars) Error in grouped_df_impl(data, unname(vars), drop): Column `weight` is unknown 2.1.4 Geometric Mean The geometric mean is the antilogarithm of \\(\\overline{\\log x}\\), where \\[\\overline{\\log x}= \\frac{1}{n}\\sum^n_{i=1}\\log{x_i}\\] As with mode, there is no function in Base-R for finding the geometric mean. # using values gm1 &lt;- function(x){ n = length(x) gm = exp((1/n)*sum(log(x))) return(gm) } gm2 &lt;- function(x){ return(exp(mean(log(x)))) } gm1(x) [1] 103.1 gm2(x) [1] 103.1 "],
["2-2-measures-of-spread.html", "2.2 Measures of Spread", " 2.2 Measures of Spread 2.2.1 Range The range is the difference between the largest and smallest observations in a sample. 2.2.2 Quantiles/Percentiles The pth percentile is defined by The (k+1)th largest sample point if np/100 is not an integer (where k is the largest integer less than np/100). The average of the (np/100)th and (np/100+1)th largest observations if np/100 is an integer. # 10th and 90th percentile quantile(x = x, probs = c(0.1,0.9)) 10% 90% 47.7 223.6 2.2.3 The Variance and Standard Deviation \\[s^2 = \\frac{\\sum^n_{i=1}(x-\\bar{x})^2}{n-1}\\] # variance var(x) [1] 5051 \\[s = \\sqrt{\\frac{\\sum^n_{i=1}(x-\\bar{x})^2}{n-1}}\\] # Standard deviation sd(x) [1] 71.07 "],
["2-3-the-coefficient-of-variation.html", "2.3 The Coefficient of Variation", " 2.3 The Coefficient of Variation The coefficient of variation (CV) is defined by \\[100\\%\\times\\frac{s}{\\bar{x}}\\] "],
["2-4-grouped-data.html", "2.4 Grouped data", " 2.4 Grouped data bwt &lt;- readr::read_csv(&quot;data/CSV/Birthweight.csv&quot;) bwt # A tibble: 100 x 1 BWT &lt;int&gt; 1 58 2 120 3 123 4 104 5 121 6 111 7 91 8 104 9 128 10 133 # ... with 90 more rows Frequency Distribution # starting dataframe (df) bwt %&gt;% # sort df by BWT column arrange(BWT) %&gt;% # counts values in BWT (n) add_count(BWT) %&gt;% # renames n to Frequency rename(Frequency = n) %&gt;% # creating new columns mutate( Cum_Percent = cume_dist(BWT) # returns cumulative percent ) %&gt;% # remove duplicated rows distinct(.) -&gt; freq_tab DT::datatable(freq_tab) "],
["2-5-graphic-methods.html", "2.5 Graphic Methods", " 2.5 Graphic Methods 2.5.1 Bar Graphs Base-R hist(bwt$BWT) ggplot2 library(ggplot2) ggplot(data = bwt,aes(BWT))+ geom_histogram(fill = &quot;white&quot;, color = &quot;black&quot;,binwidth = 10)+ ylab(&quot;Count&quot;) 2.5.2 Stem-and-Leaf Plots Base-R stem(bwt$BWT, scale = 2) The decimal point is 1 digit(s) to the right of the | 3 | 2 4 | 5 | 8 6 | 478 7 | 8 | 3556788999 9 | 12344568889 10 | 0123444445567888899 11 | 00122235555556889 12 | 01112222344445567788 13 | 222334557888 14 | 0146 15 | 5 16 | 1 2.5.3 Box Plots Base-R boxplot(bwt$BWT) ggplot2 ggplot(bwt, aes(x = &quot;&quot;,BWT))+geom_boxplot() "],
["3-probability.html", "3 Probability ", " 3 Probability "],
["3-1-introduction.html", "3.1 Introduction", " 3.1 Introduction "],
["3-2-definition-of-probability.html", "3.2 Definition of Probability", " 3.2 Definition of Probability "],
["3-3-some-useful-probabilistic-notation.html", "3.3 Some Useful Probabilistic notation", " 3.3 Some Useful Probabilistic notation "],
["3-4-the-multiplication-law-of-probability.html", "3.4 The Multiplication Law of Probability", " 3.4 The Multiplication Law of Probability "],
["3-5-the-addition-law-of-probability.html", "3.5 The Addition Law of Probability", " 3.5 The Addition Law of Probability "],
["3-6-conditional-probability.html", "3.6 Conditional Probability", " 3.6 Conditional Probability "],
["3-7-bayes-rule-and-screening-tests.html", "3.7 Bayes’ Rule and Screening Tests", " 3.7 Bayes’ Rule and Screening Tests "],
["3-8-bayesian-inference.html", "3.8 Bayesian inference", " 3.8 Bayesian inference "],
["3-9-roc-curves.html", "3.9 RoC Curves", " 3.9 RoC Curves "],
["3-10-prevalence-and-incidence.html", "3.10 Prevalence and incidence", " 3.10 Prevalence and incidence "],
["4-discrete-probability-distributions.html", "4 Discrete Probability distributions ", " 4 Discrete Probability distributions "],
["4-1-introduction-1.html", "4.1 Introduction", " 4.1 Introduction "],
["4-2-random-variables.html", "4.2 Random Variables", " 4.2 Random Variables "],
["4-3-the-probability-mass-function-for-a-discrete-random-variable.html", "4.3 The Probability-Mass Function for a Discrete Random Variable", " 4.3 The Probability-Mass Function for a Discrete Random Variable "],
["4-4-the-expected-value-of-a-discrete-random-variable.html", "4.4 The Expected Value of a discrete Random Variable", " 4.4 The Expected Value of a discrete Random Variable "],
["4-5-the-variance-of-a-discrete-random-variable.html", "4.5 The Variance of a Discrete Random Variable", " 4.5 The Variance of a Discrete Random Variable "],
["4-6-the-cumulative-distribution-function-of-a-discrete-random-variable.html", "4.6 The Cumulative-Distribution Function of a Discrete Random Variable", " 4.6 The Cumulative-Distribution Function of a Discrete Random Variable "],
["4-7-permutations-and-combinations.html", "4.7 Permutations and Combinations", " 4.7 Permutations and Combinations "],
["4-8-the-binomial-distribution.html", "4.8 The Binomial distribution", " 4.8 The Binomial distribution "],
["4-9-expected-value-and-variance-of-the-binomial-distribution.html", "4.9 Expected Value and Variance of the Binomial distribution", " 4.9 Expected Value and Variance of the Binomial distribution "],
["4-10-the-poisson-distribution.html", "4.10 The Poisson distribution", " 4.10 The Poisson distribution "],
["4-11-computation-of-poisson-probabilities.html", "4.11 Computation of Poisson Probabilities", " 4.11 Computation of Poisson Probabilities "],
["4-12-expected-value-and-variance-of-the-poisson-distribution.html", "4.12 Expected Value and Variance of the Poisson Distribution", " 4.12 Expected Value and Variance of the Poisson Distribution "],
["4-13-poisson-approximation-to-the-binomial-distribution.html", "4.13 Poisson Approximation to the Binomial Distribution", " 4.13 Poisson Approximation to the Binomial Distribution "],
["5-continuous-probability-distributions.html", "5 Continuous Probability distributions ", " 5 Continuous Probability distributions "],
["5-1-introduction-2.html", "5.1 Introduction", " 5.1 Introduction "],
["5-2-general-concepts.html", "5.2 General Concepts", " 5.2 General Concepts "],
["5-3-the-normal-distribution.html", "5.3 The Normal Distribution", " 5.3 The Normal Distribution "],
["5-4-properties-of-the-standard-normal-distribution.html", "5.4 Properties of the Standard Normal Distribution", " 5.4 Properties of the Standard Normal Distribution "],
["5-5-conversion-from-an-n2-distribution-to-an-n-01-distribution.html", "5.5 Conversion from an n(μ,σ2) Distribution to an n (0,1) Distribution", " 5.5 Conversion from an n(μ,σ2) Distribution to an n (0,1) Distribution "],
["5-6-linear-combinations-of-random-variables.html", "5.6 Linear Combinations of Random Variables", " 5.6 Linear Combinations of Random Variables "],
["5-7-normal-approximation-to-the-binomial-distribution.html", "5.7 Normal Approximation to the Binomial Distribution", " 5.7 Normal Approximation to the Binomial Distribution "],
["5-8-normal-approximation-to-the-poisson-distribution.html", "5.8 Normal Approximation to the Poisson Distribution", " 5.8 Normal Approximation to the Poisson Distribution "],
["6-estimation.html", "6 Estimation ", " 6 Estimation "],
["6-1-introduction-3.html", "6.1 Introduction", " 6.1 Introduction "],
["6-2-the-relationship-between-population-and-sample.html", "6.2 The Relationship Between Population and Sample", " 6.2 The Relationship Between Population and Sample "],
["6-3-random-number-tables.html", "6.3 Random-Number Tables", " 6.3 Random-Number Tables "],
["6-4-randomized-clinical-trials.html", "6.4 Randomized Clinical Trials", " 6.4 Randomized Clinical Trials "],
["6-5-estimation-of-the-mean-of-a-distribution.html", "6.5 Estimation of the Mean of a Distribution", " 6.5 Estimation of the Mean of a Distribution "],
["6-6-estimation-of-the-variance-of-a-distribution.html", "6.6 Estimation of the Variance of a Distribution", " 6.6 Estimation of the Variance of a Distribution "],
["6-7-estimation-for-the-binomial-distribution.html", "6.7 Estimation for the Binomial Distribution", " 6.7 Estimation for the Binomial Distribution "],
["6-8-estimation-for-the-poisson-distribution.html", "6.8 Estimation for the Poisson Distribution", " 6.8 Estimation for the Poisson Distribution "],
["6-9-one-sided-confidence-intervals.html", "6.9 One-Sided Confidence Intervals", " 6.9 One-Sided Confidence Intervals "],
["6-10-the-bootstrap.html", "6.10 The Bootstrap", " 6.10 The Bootstrap "],
["7-hypothesis-testing-one-sample-inference.html", "7 hypothesis testing: one-Sample inference ", " 7 hypothesis testing: one-Sample inference "],
["7-1-introduction-4.html", "7.1 introduction", " 7.1 introduction "],
["7-2-general-concepts-1.html", "7.2 General Concepts", " 7.2 General Concepts "],
["7-3-one-sample-test-for-the-mean-of-a-normal-distribution-one-sided-alternatives.html", "7.3 one-Sample test for the Mean of a normal distribution: one-Sided Alternatives", " 7.3 one-Sample test for the Mean of a normal distribution: one-Sided Alternatives "],
["7-4-one-sample-test-for-the-mean-of-a-normal-distribution-two-sided-alternatives.html", "7.4 one-Sample test for the Mean of a normal distribution: two-Sided Alternatives", " 7.4 one-Sample test for the Mean of a normal distribution: two-Sided Alternatives "],
["7-5-the-relationship-between-hypothesis-testing-and-confidence-intervals.html", "7.5 the Relationship Between hypothesis testing and Confidence intervals", " 7.5 the Relationship Between hypothesis testing and Confidence intervals "],
["7-6-the-power-of-a-test.html", "7.6 the Power of a test", " 7.6 the Power of a test "],
["7-7-sample-size-determination.html", "7.7 Sample-Size determination", " 7.7 Sample-Size determination "],
["7-8-one-sample-2-test-for-the-variance-of-a-normal-distribution.html", "7.8 one-Sample χ2 test for the Variance of a normal distribution", " 7.8 one-Sample χ2 test for the Variance of a normal distribution "],
["7-9-one-sample-inference-for-the-binomial-distribution.html", "7.9 one-Sample inference for the Binomial distribution", " 7.9 one-Sample inference for the Binomial distribution "],
["7-10-one-sample-inference-for-the-poisson-distribution.html", "7.10 one-Sample inference for the Poisson distribution", " 7.10 one-Sample inference for the Poisson distribution "],
["8-hypothesis-testing-two-sample-inference.html", "8 Hypothesis Testing: two-Sample inference ", " 8 Hypothesis Testing: two-Sample inference "],
["8-1-introduction-5.html", "8.1 introduction", " 8.1 introduction "],
["8-2-the-paired-t-test.html", "8.2 the Paired t test", " 8.2 the Paired t test "],
["8-3-interval-estimation-for-the-comparison-of-means-from-two-paired-samples.html", "8.3 interval Estimation for the Comparison of Means from two Paired Samples", " 8.3 interval Estimation for the Comparison of Means from two Paired Samples "],
["8-4-two-sample-t-test-for-independent-samples-with-equal-variances.html", "8.4 two-Sample t test for independent Samples with Equal Variances", " 8.4 two-Sample t test for independent Samples with Equal Variances "],
["8-5-interval-estimation-for-the-comparison-of-means-from-two-independent-samples-equal-variance-case.html", "8.5 interval Estimation for the Comparison of Means from two independent Samples (Equal Variance Case)", " 8.5 interval Estimation for the Comparison of Means from two independent Samples (Equal Variance Case) "],
["8-6-testing-for-the-equality-of-two-variances.html", "8.6 testing for the Equality of two Variances", " 8.6 testing for the Equality of two Variances "],
["8-7-two-sample-t-test-for-independent-samples-with-unequal-variances.html", "8.7 two-Sample t test for independent Samples with Unequal Variances", " 8.7 two-Sample t test for independent Samples with Unequal Variances "],
["8-8-estimation-of-sample-size-and-power-for-comparing-two-means.html", "8.8 Estimation of Sample Size and Power for Comparing two Means", " 8.8 Estimation of Sample Size and Power for Comparing two Means "],
["8-9-the-treatment-of-outliers.html", "8.9 the treatment of outliers", " 8.9 the treatment of outliers "],
["8-10-derivation-of-equation-8-13.html", "8.10 derivation of Equation 8.13", " 8.10 derivation of Equation 8.13 "],
["9-nonparametric-methods.html", "9 Nonparametric Methods ", " 9 Nonparametric Methods "],
["9-1-introduction-6.html", "9.1 introduction", " 9.1 introduction "],
["9-2-the-sign-test.html", "9.2 the Sign test", " 9.2 the Sign test "],
["9-3-the-wilcoxon-signed-rank-test.html", "9.3 the Wilcoxon Signed-Rank test", " 9.3 the Wilcoxon Signed-Rank test "],
["9-4-the-wilcoxon-rank-sum-test.html", "9.4 the Wilcoxon Rank-Sum test", " 9.4 the Wilcoxon Rank-Sum test "],
["9-5-permutation-tests.html", "9.5 Permutation tests", " 9.5 Permutation tests "],
["10-hypothesis-testingcategoricaldata.html", "10 hypothesis testing:Categoricaldata ", " 10 hypothesis testing:Categoricaldata "],
["10-1-introduction-7.html", "10.1 introduction", " 10.1 introduction "],
["10-2-two-sample-test-for-binomial-proportions.html", "10.2 two-Sample test for Binomial Proportions", " 10.2 two-Sample test for Binomial Proportions "],
["10-3-fishers-exact-test.html", "10.3 Fisher’s Exact test", " 10.3 Fisher’s Exact test "],
["10-4-two-sample-test-for-binomial-proportions-for-matched-pair-data-mcnemars-test.html", "10.4 two-Sample test for Binomial Proportions for Matched-Pair data (Mcnemar’s test)", " 10.4 two-Sample test for Binomial Proportions for Matched-Pair data (Mcnemar’s test) "],
["10-5-estimation-of-sample-size-and-power-for-comparing-two-binomial-proportions.html", "10.5 Estimation of Sample Size and Power for Comparing two Binomial Proportions", " 10.5 Estimation of Sample Size and Power for Comparing two Binomial Proportions "],
["10-6-r-c-contingency-tables.html", "10.6 R × C Contingency tables", " 10.6 R × C Contingency tables "],
["10-7-chi-square-goodness-of-fit-test.html", "10.7 Chi-Square Goodness-of-Fit test", " 10.7 Chi-Square Goodness-of-Fit test "],
["10-8-the-kappa-statistic.html", "10.8 the Kappa Statistic", " 10.8 the Kappa Statistic "],
["10-9-derivation-of-selected-formulas.html", "10.9 derivation of Selected Formulas", " 10.9 derivation of Selected Formulas "],
["11-regression-and-correlation-methods.html", "11 Regression and Correlation Methods ", " 11 Regression and Correlation Methods "],
["11-1-introduction-8.html", "11.1 introduction", " 11.1 introduction "],
["11-2-general-concepts-2.html", "11.2 General Concepts", " 11.2 General Concepts "],
["11-3-fitting-regression-linesthe-method-of-least-squares.html", "11.3 Fitting Regression Lines—the Method of Least Squares", " 11.3 Fitting Regression Lines—the Method of Least Squares "],
["11-4-inferences-about-parameters-from-regression-lines.html", "11.4 inferences About Parameters from Regression Lines", " 11.4 inferences About Parameters from Regression Lines "],
["11-5-interval-estimation-for-linear-regression.html", "11.5 interval Estimation for Linear Regression", " 11.5 interval Estimation for Linear Regression "],
["11-6-assessing-the-goodness-of-fit-of-regression-lines.html", "11.6 Assessing the Goodness of Fit of Regression Lines", " 11.6 Assessing the Goodness of Fit of Regression Lines "],
["11-7-the-correlation-coefficient.html", "11.7 the Correlation Coefficient", " 11.7 the Correlation Coefficient "],
["11-8-statistical-inference-for-correlation-coefficients.html", "11.8 Statistical inference for Correlation Coefficients", " 11.8 Statistical inference for Correlation Coefficients "],
["11-9-multiple-regression.html", "11.9 Multiple Regression", " 11.9 Multiple Regression "],
["11-10-partial-and-multiple-correlation.html", "11.10 Partial and Multiple Correlation", " 11.10 Partial and Multiple Correlation "],
["11-11-rank-correlation.html", "11.11 Rank Correlation", " 11.11 Rank Correlation "],
["11-12-interval-estimation-for-rank-correlation-coefficients.html", "11.12 interval Estimation for Rank-Correlation Coefficients", " 11.12 interval Estimation for Rank-Correlation Coefficients "],
["11-13-derivation-of-equation-11-26.html", "11.13 derivation of Equation 11.26", " 11.13 derivation of Equation 11.26 "],
["12-multisample-inference.html", "12 Multisample inference ", " 12 Multisample inference "],
["12-1-introduction-to-the-one-way-analysis-of-variance.html", "12.1 introduction to the one-Way Analysis of Variance", " 12.1 introduction to the one-Way Analysis of Variance "],
["12-2-one-way-anovafixed-effects-model.html", "12.2 one-Way AnoVA—Fixed-Effects Model", " 12.2 one-Way AnoVA—Fixed-Effects Model "],
["12-3-hypothesis-testing-in-one-way-anova-fixed-effects-model.html", "12.3 hypothesis testing in one-Way AnoVA— Fixed-Effects Model", " 12.3 hypothesis testing in one-Way AnoVA— Fixed-Effects Model "],
["12-4-comparisons-of-specific-groups-in-one-way-anova.html", "12.4 Comparisons of Specific Groups in one- Way AnoVA", " 12.4 Comparisons of Specific Groups in one- Way AnoVA "],
["12-5-two-way-anova.html", "12.5 two-Way AnoVA", " 12.5 two-Way AnoVA "],
["12-6-the-kruskal-wallis-test.html", "12.6 the Kruskal-Wallis test", " 12.6 the Kruskal-Wallis test "],
["12-7-one-way-anovathe-random-effects-model.html", "12.7 one-Way AnoVA—the Random-Effects Model", " 12.7 one-Way AnoVA—the Random-Effects Model "],
["12-8-the-intraclass-correlation-coefficient.html", "12.8 the intraclass Correlation Coefficient", " 12.8 the intraclass Correlation Coefficient "],
["12-9-mixed-models.html", "12.9 Mixed Models", " 12.9 Mixed Models "],
["12-10-derivation-of-equation.html", "12.10 derivation of Equation", " 12.10 derivation of Equation "],
["13-design-and-analysis-techniques-for-epidemiologic-studies.html", "13 design and Analysis techniques for Epidemiologic Studies ", " 13 design and Analysis techniques for Epidemiologic Studies "],
["13-1-introduction-9.html", "13.1 introduction", " 13.1 introduction "],
["13-2-study-design.html", "13.2 Study design", " 13.2 Study design "],
["13-3-measures-of-effect-for-categorical-data.html", "13.3 Measures of Effect for Categorical data", " 13.3 Measures of Effect for Categorical data "],
["13-4-attributable-risk.html", "13.4 Attributable Risk", " 13.4 Attributable Risk "],
["13-5-confounding-and-standardization.html", "13.5 Confounding and Standardization", " 13.5 Confounding and Standardization "],
["13-6-methods-of-inference-for-stratified-categorical-datathe-mantel-haenszel-test.html", "13.6 Methods of inference for Stratified Categorical data—the Mantel-haenszel test", " 13.6 Methods of inference for Stratified Categorical data—the Mantel-haenszel test "],
["13-7-multiple-logistic-regression.html", "13.7 Multiple Logistic Regression", " 13.7 Multiple Logistic Regression "],
["13-8-extensions-to-logistic-regression.html", "13.8 Extensions to Logistic Regression", " 13.8 Extensions to Logistic Regression "],
["13-9-sample-size-estimation-for-logistic-regression.html", "13.9 Sample Size Estimation for Logistic Regression", " 13.9 Sample Size Estimation for Logistic Regression "],
["13-10-meta-analysis.html", "13.10 Meta-Analysis", " 13.10 Meta-Analysis "],
["13-11-equivalence-studies.html", "13.11 Equivalence Studies", " 13.11 Equivalence Studies "],
["13-12-the-cross-over-design.html", "13.12 the Cross-over design", " 13.12 the Cross-over design "],
["13-13-clustered-binary-data.html", "13.13 Clustered Binary data", " 13.13 Clustered Binary data "],
["13-14-longitudinal-data-analysis.html", "13.14 Longitudinal data Analysis", " 13.14 Longitudinal data Analysis "],
["13-15-measurement-error-methods.html", "13.15 Measurement-Error Methods", " 13.15 Measurement-Error Methods "],
["13-16-missing-data.html", "13.16 Missing data", " 13.16 Missing data "],
["13-17-derivation-of-100-1-ci-for-the-risk-difference.html", "13.17 derivation of 100% × (1 – α) Ci for the Risk difference", " 13.17 derivation of 100% × (1 – α) Ci for the Risk difference "],
["14-hypothesis-testing-person-time-data.html", "14 Hypothesis testing: Person-time data ", " 14 Hypothesis testing: Person-time data "],
["14-1-measure-of-effect-for-person-time-data.html", "14.1 Measure of Effect for Person-time data", " 14.1 Measure of Effect for Person-time data "],
["14-2-one-sample-inference-for-incidence-rate-data.html", "14.2 one-Sample inference for incidence-Rate data", " 14.2 one-Sample inference for incidence-Rate data "],
["14-3-two-sample-inference-for-incidence-rate-data.html", "14.3 two-Sample inference for incidence-Rate data", " 14.3 two-Sample inference for incidence-Rate data "],
["14-4-power-and-sample-size-estimation-for-person-time-data.html", "14.4 Power and Sample-Size Estimation for Person-time data", " 14.4 Power and Sample-Size Estimation for Person-time data "],
["14-5-inference-for-stratified-person-time-data.html", "14.5 Inference for Stratified Person-Time Data", " 14.5 Inference for Stratified Person-Time Data "],
["14-6-power-and-sample-size-estimation-for-stratified-person-time-data.html", "14.6 Power and Sample-Size Estimation for Stratified Person-Time Data", " 14.6 Power and Sample-Size Estimation for Stratified Person-Time Data "],
["14-7-testing-for-trend-incidence-rate-data.html", "14.7 Testing for Trend: Incidence-Rate Data", " 14.7 Testing for Trend: Incidence-Rate Data "],
["14-8-introduction-to-survival-analysis.html", "14.8 Introduction to Survival Analysis", " 14.8 Introduction to Survival Analysis "],
["14-9-estimation-of-survival-curves-the-kaplan-meier-estimator.html", "14.9 Estimation of Survival Curves: The Kaplan-Meier Estimator", " 14.9 Estimation of Survival Curves: The Kaplan-Meier Estimator "],
["14-10-the-log-rank-test.html", "14.10 The Log-Rank Test", " 14.10 The Log-Rank Test "],
["14-11-the-proportional-hazards-model.html", "14.11 The Proportional-Hazards Model", " 14.11 The Proportional-Hazards Model "],
["14-12-power-and-sample-size-estimation-under-the-proportional-hazards-model.html", "14.12 Power and Sample-Size Estimation under the Proportional-Hazards Model", " 14.12 Power and Sample-Size Estimation under the Proportional-Hazards Model "],
["14-13-parametric-survival-analysis.html", "14.13 Parametric Survival Analysis", " 14.13 Parametric Survival Analysis "],
["14-14-parametric-regression-models-for-survival-data.html", "14.14 Parametric Regression Models for Survival Data", " 14.14 Parametric Regression Models for Survival Data "],
["14-15-derivation-of-selected-formulas-1.html", "14.15 Derivation of Selected Formulas", " 14.15 Derivation of Selected Formulas "]
]
