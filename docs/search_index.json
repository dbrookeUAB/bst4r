[
["index.html", "Biostatistics using R Preface", " Biostatistics using R Dewey Brooke 2018-09-03 Preface "],
["1-intro.html", "1 Introduction", " 1 Introduction Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. "],
["1-1-the-r-ecosystem.html", "1.1 The R Ecosystem", " 1.1 The R Ecosystem Base-R Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. The Comprehensive R Archive Network Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Github Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. "],
["1-2-getting-started.html", "1.2 Getting Started", " 1.2 Getting Started Installing the binary distribution of R should be done before installing RStudio (discussed later) although it won’t hurt anything if you did that by accident. If you installed RStudio first, you will likely see an error window after launching RStudio that will ask to take you to the CRAN website to install R. Follow the instructions under the heading of the operating system you are using in the next section. 1.2.1 Installing R 1.2.1.1 Mac Visit the [CRAN website]https://cran.rstudio.com/bin/macosx/) and click the latest release or you can simply click the following link to download directly. If your are using a legacy Mac operating system, you can install one of the Previous releases although some R packages may not be compatible. 1.2.1.2 Windows Visit the CRAN website and click Download R 3.5.1 for Windows or simply click the following link to download directly. The installation instructions from the CRAN website state: The distribution is distributed as an installer R-3.5.1-win.exe. Just run this for a Windows-style installer. For more details, including command-line options for the installer andhow to uninstall, see the rw-FAQ. If you are reading this as part of an installed version of R, that is both a file rw-FAQ in this directory and a file doc-FAQ.html. 1.2.1.3 Linux To be updated later…. 1.2.2 Install RStudio After you install R on your machine, you can immediately start using the R terminal, yet the book will focus on using the RStudio IDE. However, a special section in the Appendix will give an overview on how to use the popular multilanguage-datascience application called Jupyter. Install RStudio by first visiting their website and begin installing RStudio Desktop (Open Source License). "],
["1-3-quick-introduction-to-rstudio.html", "1.3 Quick Introduction to RStudio", " 1.3 Quick Introduction to RStudio 1.3.1 The Interface Figure 1.1: The main four panel window. A.) Source Panel B.) Console C.)Enviornment, History, Connections, etc D.) Files, Plots, Packages, Help, Viewer. This layout can be changed in the View menu. "],
["1-4-starting-a-project.html", "1.4 Starting a Project", " 1.4 Starting a Project knitr::include_graphics(&quot;images/new_project_00.png&quot;) knitr::include_graphics(&quot;images/new_project_01.png&quot;) knitr::include_graphics(&quot;images/new_project_02.png&quot;) knitr::include_graphics(&quot;images/new_project_03.png&quot;) "],
["1-5-understanding-the-basics.html", "1.5 Understanding the basics", " 1.5 Understanding the basics If you are familiar with excel, you probably at some point had to set the format of whatever you had placed in that cell, especially when excel prevented your calculations by treating your set of numbers as dates or even worse as text. In R however, cells are referred to as objects and we refer to their format as their type or mode. "],
["reading-data-files-into-r.html", "Reading Data Files into R", " Reading Data Files into R The first step in every analysis requires data to be read into the environment, and learning how to do this is the first hurdle a person needs to overcome to begin learning to use R. Data can exist in many different formats, either as the generic universal types (e.g. csv, tsv, .json, etc) or software specific types (e.g. .xlsx, `` ) In this chapter, we will first discuss how to read data using functions in Base-R (when possible), and then we will discuss alternative packages, such as the multitude of packages in the Tidyverse, and highlight their advantages over Base-R functions. 1.5.1 Generic Formats 1.5.1.1 CSV- Comma Separated Values The fields are separated by a comma , and are typically used for loading into spreadsheets. For example: csv_example_path &lt;- &quot;data/ASCII-comma/FEV.DAT.txt&quot; readLines(csv_example_path)[1:8] # reads each line of the file [1] &quot;&#39;Id&#39;,&#39;Age&#39;,&#39;FEV&#39;,&#39;Hgt&#39;,&#39;Sex&#39;,&#39;Smoke&#39;&quot; [2] &quot;301,9,1.708,57,0,0&quot; [3] &quot;451,8,1.724,67.5,0,0&quot; [4] &quot;501,7,1.72,54.5,0,0&quot; [5] &quot;642,9,1.558,53,1,0&quot; [6] &quot;901,9,1.895,57,1,0&quot; [7] &quot;1701,8,2.336,61,0,0&quot; [8] &quot;1752,6,1.919,58,0,0&quot; # Note: readLines(csv_example_path) is the same as # readLines(&quot;data/ASCII-comma/FEV.DAT.txt&quot;) In Base-R, CSV data can be read using the read.csv() function. The read.csv2() function is used in countries that use a comma as a decimal point and a semicolon as a field separator. csv_example &lt;- read.csv(csv_example_path) head(csv_example) X.Id. X.Age. X.FEV. X.Hgt. X.Sex. X.Smoke. 1 301 9 1.708 57.0 0 0 2 451 8 1.724 67.5 0 0 3 501 7 1.720 54.5 0 0 4 642 9 1.558 53.0 1 0 5 901 9 1.895 57.0 1 0 6 1701 8 2.336 61.0 0 0 1.5.1.2 TSV- Tab Separated Values The fields are separated by a tabulation or and are saved as .txt files. However, not all .txt files contain tab separated values. For example: tsv_example_path &lt;- &quot;data/ASCII-tab/FEV.DAT.txt&quot; readLines(tsv_example_path)[1:8] [1] &quot;&#39;Id&#39;\\t&#39;Age&#39;\\t&#39;FEV&#39;\\t&#39;Hgt&#39;\\t&#39;Sex&#39;\\t&#39;Smoke&#39;&quot; [2] &quot;301\\t9\\t1.708\\t57\\t0\\t0&quot; [3] &quot;451\\t8\\t1.724\\t67.5\\t0\\t0&quot; [4] &quot;501\\t7\\t1.72\\t54.5\\t0\\t0&quot; [5] &quot;642\\t9\\t1.558\\t53\\t1\\t0&quot; [6] &quot;901\\t9\\t1.895\\t57\\t1\\t0&quot; [7] &quot;1701\\t8\\t2.336\\t61\\t0\\t0&quot; [8] &quot;1752\\t6\\t1.919\\t58\\t0\\t0&quot; tsv_example &lt;- read.delim(&quot;data/ASCII-tab/FEV.DAT.txt&quot;) head(tsv_example) X.Id. X.Age. X.FEV. X.Hgt. X.Sex. X.Smoke. 1 301 9 1.708 57.0 0 0 2 451 8 1.724 67.5 0 0 3 501 7 1.720 54.5 0 0 4 642 9 1.558 53.0 1 0 5 901 9 1.895 57.0 1 0 6 1701 8 2.336 61.0 0 0 1.5.2 Excel library(readxl) 1.5.3 Software Specific Formats R is increasingly recognized as the gold standard for statistical computations, yet some of your future collaborates will exclusively use Commercial Software (SAS, SPSS, Matlab, and Stata) for their statistical computations. Although these individuals are limited by the types of files they can read or write, the haven R-package can both read and write any of these file formats. library(haven) 1.5.3.1 SAS(.sas7bdat), SPSS(.sav,.por, .xpt), Stata (.dta) sas &lt;- read_sas(&quot;data/SAS/FEV.sas7bdat&quot;) head(sas) # A tibble: 6 x 6 ID AGE FEV HGT SEX SMOKE &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 301 9 1.71 57 0 0 2 451 8 1.72 67.5 0 0 3 501 7 1.72 54.5 0 0 4 642 9 1.56 53 1 0 5 901 9 1.90 57 1 0 6 1701 8 2.34 61 0 0 spss &lt;- read_spss(&quot;data/SPSS/FEV.DAT.sav&quot;) head(spss) # A tibble: 6 x 6 Id Age FEV Hgt Sex Smoke &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 301 9 1.71 57 0 0 2 451 8 1.72 67.5 0 0 3 501 7 1.72 54.5 0 0 4 642 9 1.56 53 1 0 5 901 9 1.90 57 1 0 6 1701 8 2.34 61 0 0 stata &lt;- read_stata(&quot;data/Stata/FEV.DAT.dta&quot;) head(stata) # A tibble: 6 x 6 Id Age fev Hgt Sex Smoke &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 301 9 1.71 57 0 0 2 451 8 1.72 67.5 0 0 3 501 7 1.72 54.5 0 0 4 642 9 1.56 53 1 0 5 901 9 1.90 57 1 0 6 1701 8 2.34 61 0 0 The foreign package included in Base-R can also be used to Reading and writing data stored by some versions of ‘Epi Info’, ‘Minitab’, ‘S’, ‘SAS’, ‘SPSS’, ‘Stata’, ‘Systat’, ‘Weka’,and for reading and writing some ‘dBase’ files. 1.5.3.1.0.1 RDS rds_example &lt;- readRDS(&quot;data/RDS/BETACAR.DAT.rds&quot;) head(rds_example) # A tibble: 6 x 8 `&#39;Prepar&#39;` `&#39;Id&#39;` `&#39;Base1lvl&#39;` `&#39;Base2lvl&#39;` &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 71 298 116 2 1 73 124 146 3 1 80 176 200 4 1 83 116 180 5 1 90 152 142 6 1 92 106 106 # … with 4 more variables: `&#39;Wk6lvl&#39;` &lt;int&gt;, # `&#39;Wk8lvl&#39;` &lt;int&gt;, `&#39;Wk10lvl&#39;` &lt;int&gt;, # `&#39;Wk12lvl&#39;` &lt;int&gt; 1.5.3.1.0.2 rdata The .rdata format is R’s specific format. Instead of using a read.{something} function, .rdata is read into the environment using load(filename.rdata) and retains the original name it had when it was last saved. load(&quot;data/R/BETACAR.DAT.rdata&quot;) #named betacar when it was last saved head(betacar) Prepar Id Base1lvl Base2lvl Wk6lvl Wk8lvl Wk10lvl 1 1 71 298 116 174 178 218 2 1 73 124 146 294 278 244 3 1 80 176 200 276 286 308 4 1 83 116 180 164 238 308 5 1 90 152 142 290 300 270 6 1 92 106 106 246 206 304 Wk12lvl 1 190 2 262 3 334 4 226 5 268 6 356 "],
["2-descriptive-statistics.html", "2 Descriptive Statistics", " 2 Descriptive Statistics "],
["2-1-introduction-to-probablity.html", "2.1 Introduction to Probablity", " 2.1 Introduction to Probablity 2.1.1 Definitions Sample Space: set of all possible outcomes in an experiment or trial Event: any individual outcome of interest, or subset of outcomes of interest, in an experiment or trial Probablity: relative frequency of an event of interest over an indefintely large (or infinite) number of trials 2.1.2 Probablity The true probablity of an event is often unknown and ca only be estimated. Relative frequency probablity: Counting the number of repetitions of a process and the number of times each events occurs - Divide the number of each outcome by the total number of repetitions - This estimates the likelihood of each event occuring -This follows the ideas of bar charts and histograms 2.1.3 Sampling Population: A well defined collection of objects, such as: pregnant women, MS patients, stroke survivors, etc. - Measurements on every member of th population consitutes a census. - A census may not be feasible or desired. Sample: a subset of the populaiton from which characterstics are measured in order to estimate and infer characteristics of the population. - Individual components of the sample are its elements (observations) 2.1.4 Statistics Parameters: fixed values used to describe characteristics of a variable’s distribution (e.g., where it is centered, measures of its spread, measures of its skewness, etc.) Common paramters are the mean and standard deviation. Go back and fix *** 2.1.5 Descriptive Statistics: summary statistics used to estiamte population paramters Sample statistics include \\(\\bar{x}, s^2, s\\) - lower case letters are used for describing samples. Population parameters are estimated from the sample - population parameters include \\(\\mu, \\sigma^s, \\sigma\\) Inferential Statistics Go back and fix *** 2.1.5.1 Describing Data Data are summarized by descriptive statistics, and distributions of variables are often depicted using plots including: -charts -histograms go back and fix *** Of we have a sample of n objects, then the values are denoted as \\(x_1, x_2, ..., x_n\\). The sample mean \\[\\bar{x} = \\frac{\\sum_i^nx}{n}\\] Sample variance \\(s^2 = \\frac{\\sum^n{i}{x_i-\\bar{x}}}{n-1}\\) Sample std dev \\(s = \\sqrt{s^2}\\) It can be shown that \\(\\sum{x_i-\\bar{x}}=0\\) and the actual differences between each observation and the samle mean are not informative. The square distance between the observed values and the sample mean is what we examine. Alternative measure is the mean obsolute deviation (MAD) \\[\\text{MAD} = \\frac{\\sum^n_{i=1}|x_i-\\bar{x}|}{n}\\] Median: the middle value in the list of ordered values (the median is the \\(50^\\text{th}\\) percentile) Percentile: the value such that some percent of the data are less than that value. - for the ordered data, the \\(k^\\text{th}\\) percentile is in the \\(\\frac{k}{100}(1+n)\\) position. - if the position falls between 2 numbers, then the \\(k^\\text{th}\\) percentile is the average of the two surrounding numbers. quartiles: the \\(25^\\text{th}\\) (\\(Q_1\\)) and \\(75^\\text{th}\\) (\\(Q_3\\)) interquartile range: \\(75^\\text{th}-50^\\text{th}\\) percentile "],
["2-2-measures-of-location-using-base-r.html", "2.2 Measures of Location using Base R", " 2.2 Measures of Location using Base R Determining the correct method for measuring the central tendancy of a vector depends on the relationship between the numbers within the vector. Numbers that can be summed in a linear sequence are best represented using the arithmic mean. If you’re measuring units that add up as reciprocals in a sequence (such as speed or distance / time over a constant distance, capacitance in series, resistance in parallel), then a harmonic mean will give you a meaningful average. For example, the harmonic mean of capacitors in series represents the capacitance that a single capacitor would have if only one capacitor was used instead of the set of capacitors in series. If you’re measuring units that multiply in a sequence (such as growth rates or percentages), then a geometric mean will give you a meaningful average. For example, the geometric mean of a sequence of different annual interest rates over 10 years represents an interest rate that, if applied constantly for ten years, would produce the same amount growth in principal as the sequence of different annual interest rates over ten years did. Does an arithmetic mean of interest rates have any significance? As a number, sure. But as an “average” interest rate it seems less intuitive because the principal it produces at the end of ten years is much larger than the geometric mean. Similarly, the harmonic mean of interest rates produces a smaller principal, and so is less intuitive. Now consider areas and volumes as a test of understanding. What mean should we use to report the “average” area or volume in a sequence of areas or volumes? Area is measured in units of length squared. Volume is measured in units of length cubed. In a sequence of areas or volumes, we could either add them up linearly and divide or multiply them and take the roots — which is correct? It depends on what we’re measuring. If these areas or volumes are dependent upon each other (e.g., the size of the same microbe at different times), then a geometric mean probably makes more sense. If these areas or volumes are independent of each other (e.g., the size of a house or pool), then an arithmetic mean probably makes more sense. But whatever you decide, when in doubt report that decision. There is nothing worse for a reader than to see an “average” and not know how it was calculated! - Michael F. Martin,Quora Answer 2.2.1 The Arithmetic Mean The arithmetic mean is the sum of all the observations divided by the number of observations. It is written in statistical terms as \\[\\overline{x} = \\frac{1}{n}\\sum^n_{i=1}x_i\\] mean(ChickWeight$weight) [1] 121.8 2.2.2 The Median The sample median is: If n is odd \\(\\rightarrow\\) \\(\\Big(\\frac{n+1}{2}\\Big)\\text{th}\\) largest observation If n is even \\(\\rightarrow\\) \\(\\Big(\\frac{n}{2}\\Big)\\text{th}\\) and \\(\\Big(\\frac{n}{2}+1\\Big)\\text{th}\\) largest observations median(ChickWeight$weight) [1] 103 2.2.3 The Mode The mode is the most frequently occurring value among all observations in the sample. Although it is infrequently used, it is very useful for categorical and discrete data. Since there isn’t a built in R-function for mode, we learn how to write a function to return the mode through a few examples. 2.2.3.1 Functions 2.2.3.1.1 Base R Example The most simple function begins by assigning the output of function() to some character string (e.g. simple_fun) All statements after the function() are referred as the body of the function. function_name &lt;- function(arg1, arg2,...) { #statements return(&quot;some output&quot;) } function_name() # returns NULL [1] &quot;some output&quot; Use return() to output the result of the function. return_value &lt;- function(x,y) { z=x-y z=x+y return(z) } return_value(4,5) [1] 9 Since our goal is to find the most frequently occurring value in our data-set (ChickWeight), we need to decide the sequence of functions that we need to accomplish this. As you continue to add various R functions to your R tool belt, you will find many possible combinations for the same solution. First, let’s assign the weight column from ChickWeight to x to simplify things. When x is called, the weight column from ChickWeight is returned as a vector. x&lt;-ChickWeight$weight head(x) [1] 42 51 59 64 76 93 We can return the size of x using the length function. 578 length(x) [1] 578 We can reduce x to return only the unique values by using the unique function. We’ll assign it to y so we can use it later. y &lt;- unique(x) length(y) [1] 212 To more easily watch how the functions are working, we will create two data-frames to watch how we are manipulating both x and y. df.x &lt;- data.frame(x) df.y &lt;- data.frame(y) Using the unique values from the x vector we defined as y, we can use the match function to return a vector that replaces each value in x with their position in the y vector (1-212). df.x$position_in_y&lt;-match(x, y) head(df.x, n = 30) x position_in_y 1 42 1 2 51 2 3 59 3 4 64 4 5 76 5 6 93 6 7 106 7 8 125 8 9 149 9 10 171 10 11 199 11 12 205 12 13 40 13 14 49 14 15 58 15 16 72 16 17 84 17 18 103 18 19 122 19 20 138 20 21 162 21 22 187 22 23 209 23 24 215 24 25 43 25 26 39 26 27 55 27 28 67 28 29 84 17 30 99 29 The output from match can then be simplified using the tabulate function df.y$frequency &lt;- tabulate(df.x$position_in_y) head(df.y) y frequency 1 42 15 2 51 8 3 59 5 4 64 5 5 76 3 6 93 4 which.max returns the position of the maximum value. which.max(df.y$frequency) [1] 43 df.y[43,] #df.y[row,column] y frequency 43 41 20 Putting it all together, we can do this in one line. df.y[which.max(tabulate(match(x,y))),] y frequency 43 41 20 y[which.max(tabulate(match(x,y)))] [1] 41 Writing this as a function mode &lt;- function(x){ unique_x &lt;- unique(x) result&lt;-unique_x[which.max(tabulate(match(x,unique_x)))] return(result) } mode(x) [1] 41 2.2.3.1.2 Tidyverse Example As with most problems in R, we can also find a solution using packages from the Tidyverse. We will therefore use this as an opportunity to introduce some of the basic tenants of Tidyverse functions. In the dplyr package, a typical workflow will combine observations into a single data-frame, aggregate them into groups, manipulate values into new columns, and summaries the data-frame into more simple terms. The piping operator %&gt;% allows for this to be done seamlessly by literally pipping the result of one function into arguments of another function. print(&quot;non-piped text&quot;) [1] &quot;non-piped text&quot; library(dplyr) &quot;piped text&quot; %&gt;% print() [1] &quot;piped text&quot; To show how this works, we will start with a simple example where we first want to divided the sum of three and some other number (e.g. 2) by seven. Because of the order of operations, the sum of two and three would need to be placed with parenthesis to indicate it happens before dividing by seven. (4+3)/7 # correct [1] 1 4 + 3 / 7 # incorrect [1] 4.429 The piping operator allows the order of operations be explicated dictated with manipulations of starting value reading from the left to right. # pipes use the (.) as a placeholder 4 %&gt;% + 3 %&gt;% {./7} # removing the { } returns an error [1] 1 Using pipes increases readability of your R-code and it can easily be reused for different starting values. In R Studio, the pipe character can be easily inserted using a keyboard shortcut (Windows:Ctrl+Shift+M, Mac:Cmd+Shift+M). 11 %&gt;% + 3 %&gt;% {./7} [1] 2 Plus, the piped workflow can easily be defined by a function by assigning it to some string with a . in the beginning. op_order &lt;- . %&gt;% +3 %&gt;% {./7} op_order(4) [1] 1 op_order(11) [1] 2 2.2.3.1.2.1 Determining Mode with dplyr Using the ChickWeight data-set as before, we start by outlining the order of operations. Group the data by weights group_by() Tally the number of members within each group and sort by frequency. tally() Select the row with the largest n. slice() Return the corresponding weight. .$weight ChickWeight %&gt;% group_by(weight) %&gt;% tally(sort = TRUE) %&gt;% slice(1) %&gt;% .$weight [1] 41 As before, this workflow can be written as a function by placing . between the assignment operator &lt;- and piping operator %&gt;%. mode_cw&lt;-. %&gt;% group_by(weight) %&gt;% tally(sort = TRUE) %&gt;% slice(1) %&gt;% .$weight mode_cw(ChickWeight) [1] 41 However, this function will only work on the ChickWeight data-set. mode_cw(mtcars) Error in grouped_df_impl(data, unname(vars), drop): Column `weight` is unknown 2.2.4 Geometric Mean The geometric mean is the antilogarithm of \\(\\overline{\\log x}\\), where \\[\\overline{\\log x}= \\frac{1}{n}\\sum^n_{i=1}\\log{x_i}\\] As with mode, there is no function in Base-R for finding the geometric mean. # using values gm1 &lt;- function(x){ n = length(x) gm = exp((1/n)*sum(log(x))) return(gm) } gm2 &lt;- function(x){ return(exp(mean(log(x)))) } gm1(x) [1] 103.1 gm2(x) [1] 103.1 "],
["2-3-measures-of-spread.html", "2.3 Measures of Spread", " 2.3 Measures of Spread 2.3.1 Range The range is the difference between the largest and smallest observations in a sample. 2.3.2 Quantiles/Percentiles The pth percentile is defined by The (k+1)th largest sample point if np/100 is not an integer (where k is the largest integer less than np/100). The average of the (np/100)th and (np/100+1)th largest observations if np/100 is an integer. # 10th and 90th percentile quantile(x = x, probs = c(0.1,0.9)) 10% 90% 47.7 223.6 2.3.3 The Variance and Standard Deviation \\[s^2 = \\frac{\\sum^n_{i=1}(x-\\bar{x})^2}{n-1}\\] # variance var(x) [1] 5051 \\[s = \\sqrt{\\frac{\\sum^n_{i=1}(x-\\bar{x})^2}{n-1}}\\] # Standard deviation sd(x) [1] 71.07 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD The standard deviation is invariant to location. All data values can be shifted up or down by a constant, c, and the variance and standard deviation will remain the same. If \\(x_1, x_2, ..., x_n\\) are all multipled by a constant \\(c\\), we have \\(cx_1, cx_2, ..., cx_n\\), and the varaince is now \\(c^2s^2\\) while the \\(s^2\\) is the variance of the original values. ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; be36a580c91456396a6159d2a5434c00d94ce164 ### The Coefficient of Variation The coefficient of variation (CV) is defined by \\[100\\%\\times\\frac{s}{\\bar{x}}\\] 2.3.4 Example: Birthweights library(haven) btw &lt;- read_sas(&quot;data/SAS/birthweight.sas7bdat&quot;) btw # A tibble: 100 x 1 BWT &lt;dbl&gt; 1 32 2 58 3 64 4 67 5 68 6 83 7 85 8 85 9 86 10 87 # … with 90 more rows summary(btw) BWT Min. : 32.0 1st Qu.: 98.8 Median :112.0 Mean :111.3 3rd Qu.:124.2 Max. :161.0 library(skimr) skim(btw) Skim summary statistics n obs: 100 n variables: 1 ── Variable type:numeric ───────────────────────────────────────────────────────── variable missing complete n mean sd p0 p25 BWT 0 100 100 111.26 20.95 32 98.75 p50 p75 p100 hist 112 124.25 161 ▁▁▁▅▇▇▃▁ Ordering data "],
["2-4-grouped-data.html", "2.4 Grouped data", " 2.4 Grouped data bwt &lt;- readr::read_csv(&quot;data/CSV/Birthweight.csv&quot;) bwt # A tibble: 100 x 1 BWT &lt;dbl&gt; 1 58 2 120 3 123 4 104 5 121 6 111 7 91 8 104 9 128 10 133 # … with 90 more rows Frequency Distribution # starting dataframe (df) bwt %&gt;% # sort df by BWT column arrange(BWT) %&gt;% # counts values in BWT (n) add_count(BWT) %&gt;% # renames n to Frequency rename(Frequency = n) %&gt;% # creating new columns mutate( Cum_Percent = cume_dist(BWT) # returns cumulative percent ) %&gt;% # remove duplicated rows distinct(.) -&gt; freq_tab DT::datatable(freq_tab) "],
["2-5-graphic-methods.html", "2.5 Graphic Methods", " 2.5 Graphic Methods 2.5.1 Bar Graphs Base-R hist(bwt$BWT) ggplot2 library(ggplot2) ggplot(data = bwt,aes(BWT))+ geom_histogram(fill = &quot;white&quot;, color = &quot;black&quot;,binwidth = 10)+ ylab(&quot;Count&quot;) 2.5.2 Stem-and-Leaf Plots Base-R stem(bwt$BWT, scale = 2) The decimal point is 1 digit(s) to the right of the | 3 | 2 4 | 5 | 8 6 | 478 7 | 8 | 3556788999 9 | 12344568889 10 | 0123444445567888899 11 | 00122235555556889 12 | 01112222344445567788 13 | 222334557888 14 | 0146 15 | 5 16 | 1 2.5.3 Box Plots Base-R boxplot(bwt$BWT) ggplot2 ggplot(bwt, aes(x = &quot;&quot;,BWT))+geom_boxplot() "],
["3-probability.html", "3 Probability", " 3 Probability The probability of an event is the relative frequency of an event (outcome) within a sample space (all possible outcomes). 3.0.1 Some Useful Probabilistic notation Symbol Definition \\(\\cap\\) intersection (and) \\(\\cup\\) union (or) A’ \\(\\bar{A}\\) \\(A^c\\) complement (not) { } event \\(Pr(A)\\) Probability that event A will occur. \\(Pr(A\\|B)\\) The probability of A occurring if B occurred. \\(P(A&#39;)\\) The probability A will not occur. (\\(\\bar{A}=1-Pr(A)\\)) \\(Pr(A\\cap B)\\) The Probability events A and B occurring together. \\(Pr(A\\cup B)\\) Probability events either A or B occur, or they both occurr. The probability of an event (\\(Pr(E)\\)) always satisfies: \\[0 \\leq \\text{Pr(E)} \\leq 1\\] If events A and B cannot both occur simultanously, they are mutually exclusive. \\[Pr(A\\cap B)=0\\] Two events A and B are called independent if \\[Pr(A\\cap B)=Pr(A)\\times Pr(B)\\] Two events A and B are called dependent if \\[Pr(A\\cap B)\\ne Pr(A)\\times Pr(B)\\] "],
["3-1-laws-of-probability.html", "3.1 Laws of Probability", " 3.1 Laws of Probability 3.1.1 The Multiplication Law of Probability: \\(Pr(A\\cap B)\\) If \\(A_1,...,A_n\\) are mutually independent events, then \\[Pr(A_1 \\cap A_2 \\cap ... \\cap A_n) = Pr(A_1)\\times Pr(A_2)\\times ...\\times Pr(A_n)\\] 3.1.2 The Addition Law of Probability: \\(Pr(A\\cup B)\\) If \\(A\\text{ and }B\\) are any events, then \\[Pr(A \\cup B) = Pr(A)+ Pr(B)- Pr(A\\cap B)\\] If \\(A\\text{ and }B\\) are mutually exclusive, then \\[Pr(A \\cup B) = Pr(A)+ Pr(B)\\] If \\(A\\text{ and }B\\) are independent events, then \\[Pr(A \\cup B) = Pr(A)+ Pr(B)\\times[1-Pr(A)]\\] "],
["3-2-conditional-probability.html", "3.2 Conditional Probability", " 3.2 Conditional Probability A conditional probability is the probability of one event if another event occurred. \\[Pr(A|B) = \\frac{Pr(A\\cap B)}{Pr(A)}\\] If A and B are independent events, then \\[Pr(B|A) = Pr(B) = Pr(B|\\bar{A})\\] If two events A and B are dependent, then \\[Pr(B|A)\\ne Pr(B) \\ne Pr(B|\\bar{A})\\] and \\[Pr(A\\cap B) \\ne Pr(A)\\times Pr(B)\\] 3.2.1 Relative Risk The relative risk (RR) of B given is \\[RR=\\frac{Pr(B|A)}{Pr(B|\\bar{A})}\\] If two events A and B are independent, then RR = 1. 3.2.2 Total Probability Let A "],
["3-3-bayes-rule-and-screening-tests.html", "3.3 Bayes’ Rule and Screening Tests", " 3.3 Bayes’ Rule and Screening Tests "],
["3-4-bayesian-inference.html", "3.4 Bayesian inference", " 3.4 Bayesian inference "],
["3-5-roc-curves.html", "3.5 RoC Curves", " 3.5 RoC Curves "],
["3-6-prevalence-and-incidence.html", "3.6 Prevalence and incidence", " 3.6 Prevalence and incidence "],
["4-discrete-probability-distributions.html", "4 Discrete Probability distributions ", " 4 Discrete Probability distributions "],
["4-1-introduction.html", "4.1 Introduction", " 4.1 Introduction "],
["4-2-random-variables.html", "4.2 Random Variables", " 4.2 Random Variables "],
["4-3-the-probability-mass-function-for-a-discrete-random-variable.html", "4.3 The Probability-Mass Function for a Discrete Random Variable", " 4.3 The Probability-Mass Function for a Discrete Random Variable "],
["4-4-the-expected-value-of-a-discrete-random-variable.html", "4.4 The Expected Value of a discrete Random Variable", " 4.4 The Expected Value of a discrete Random Variable "],
["4-5-the-variance-of-a-discrete-random-variable.html", "4.5 The Variance of a Discrete Random Variable", " 4.5 The Variance of a Discrete Random Variable "],
["4-6-the-cumulative-distribution-function-of-a-discrete-random-variable.html", "4.6 The Cumulative-Distribution Function of a Discrete Random Variable", " 4.6 The Cumulative-Distribution Function of a Discrete Random Variable "],
["4-7-permutations-and-combinations.html", "4.7 Permutations and Combinations", " 4.7 Permutations and Combinations "],
["4-8-the-binomial-distribution.html", "4.8 The Binomial distribution", " 4.8 The Binomial distribution "],
["4-9-expected-value-and-variance-of-the-binomial-distribution.html", "4.9 Expected Value and Variance of the Binomial distribution", " 4.9 Expected Value and Variance of the Binomial distribution "],
["4-10-the-poisson-distribution.html", "4.10 The Poisson distribution", " 4.10 The Poisson distribution "],
["4-11-computation-of-poisson-probabilities.html", "4.11 Computation of Poisson Probabilities", " 4.11 Computation of Poisson Probabilities "],
["4-12-expected-value-and-variance-of-the-poisson-distribution.html", "4.12 Expected Value and Variance of the Poisson Distribution", " 4.12 Expected Value and Variance of the Poisson Distribution "],
["4-13-poisson-approximation-to-the-binomial-distribution.html", "4.13 Poisson Approximation to the Binomial Distribution", " 4.13 Poisson Approximation to the Binomial Distribution "],
["5-continuous-probability-distributions.html", "5 Continuous Probability distributions ", " 5 Continuous Probability distributions "],
["5-1-introduction-1.html", "5.1 Introduction", " 5.1 Introduction "],
["5-2-general-concepts.html", "5.2 General Concepts", " 5.2 General Concepts "],
["5-3-the-normal-distribution.html", "5.3 The Normal Distribution", " 5.3 The Normal Distribution "],
["5-4-properties-of-the-standard-normal-distribution.html", "5.4 Properties of the Standard Normal Distribution", " 5.4 Properties of the Standard Normal Distribution "],
["5-5-conversion-from-an-n2-distribution-to-an-n-01-distribution.html", "5.5 Conversion from an n(μ,σ2) Distribution to an n (0,1) Distribution", " 5.5 Conversion from an n(μ,σ2) Distribution to an n (0,1) Distribution "],
["5-6-linear-combinations-of-random-variables.html", "5.6 Linear Combinations of Random Variables", " 5.6 Linear Combinations of Random Variables "],
["5-7-normal-approximation-to-the-binomial-distribution.html", "5.7 Normal Approximation to the Binomial Distribution", " 5.7 Normal Approximation to the Binomial Distribution "],
["5-8-normal-approximation-to-the-poisson-distribution.html", "5.8 Normal Approximation to the Poisson Distribution", " 5.8 Normal Approximation to the Poisson Distribution "],
["6-estimation.html", "6 Estimation ", " 6 Estimation "],
["6-1-introduction-2.html", "6.1 Introduction", " 6.1 Introduction "],
["6-2-the-relationship-between-population-and-sample.html", "6.2 The Relationship Between Population and Sample", " 6.2 The Relationship Between Population and Sample "],
["6-3-random-number-tables.html", "6.3 Random-Number Tables", " 6.3 Random-Number Tables "],
["6-4-randomized-clinical-trials.html", "6.4 Randomized Clinical Trials", " 6.4 Randomized Clinical Trials "],
["6-5-estimation-of-the-mean-of-a-distribution.html", "6.5 Estimation of the Mean of a Distribution", " 6.5 Estimation of the Mean of a Distribution "],
["6-6-estimation-of-the-variance-of-a-distribution.html", "6.6 Estimation of the Variance of a Distribution", " 6.6 Estimation of the Variance of a Distribution "],
["6-7-estimation-for-the-binomial-distribution.html", "6.7 Estimation for the Binomial Distribution", " 6.7 Estimation for the Binomial Distribution "],
["6-8-estimation-for-the-poisson-distribution.html", "6.8 Estimation for the Poisson Distribution", " 6.8 Estimation for the Poisson Distribution "],
["6-9-one-sided-confidence-intervals.html", "6.9 One-Sided Confidence Intervals", " 6.9 One-Sided Confidence Intervals "],
["6-10-the-bootstrap.html", "6.10 The Bootstrap", " 6.10 The Bootstrap "],
["7-hypothesis-testing-one-sample-inference.html", "7 hypothesis testing: one-Sample inference ", " 7 hypothesis testing: one-Sample inference "],
["7-1-introduction-3.html", "7.1 introduction", " 7.1 introduction "],
["7-2-general-concepts-1.html", "7.2 General Concepts", " 7.2 General Concepts "],
["7-3-one-sample-test-for-the-mean-of-a-normal-distribution-one-sided-alternatives.html", "7.3 one-Sample test for the Mean of a normal distribution: one-Sided Alternatives", " 7.3 one-Sample test for the Mean of a normal distribution: one-Sided Alternatives "],
["7-4-one-sample-test-for-the-mean-of-a-normal-distribution-two-sided-alternatives.html", "7.4 one-Sample test for the Mean of a normal distribution: two-Sided Alternatives", " 7.4 one-Sample test for the Mean of a normal distribution: two-Sided Alternatives "],
["7-5-the-relationship-between-hypothesis-testing-and-confidence-intervals.html", "7.5 the Relationship Between hypothesis testing and Confidence intervals", " 7.5 the Relationship Between hypothesis testing and Confidence intervals "],
["7-6-the-power-of-a-test.html", "7.6 the Power of a test", " 7.6 the Power of a test "],
["7-7-sample-size-determination.html", "7.7 Sample-Size determination", " 7.7 Sample-Size determination "],
["7-8-one-sample-2-test-for-the-variance-of-a-normal-distribution.html", "7.8 one-Sample χ2 test for the Variance of a normal distribution", " 7.8 one-Sample χ2 test for the Variance of a normal distribution "],
["7-9-one-sample-inference-for-the-binomial-distribution.html", "7.9 one-Sample inference for the Binomial distribution", " 7.9 one-Sample inference for the Binomial distribution "],
["7-10-one-sample-inference-for-the-poisson-distribution.html", "7.10 one-Sample inference for the Poisson distribution", " 7.10 one-Sample inference for the Poisson distribution "],
["8-hypothesis-testing-two-sample-inference.html", "8 Hypothesis Testing: two-Sample inference ", " 8 Hypothesis Testing: two-Sample inference "],
["8-1-introduction-4.html", "8.1 introduction", " 8.1 introduction "],
["8-2-the-paired-t-test.html", "8.2 the Paired t test", " 8.2 the Paired t test "],
["8-3-interval-estimation-for-the-comparison-of-means-from-two-paired-samples.html", "8.3 interval Estimation for the Comparison of Means from two Paired Samples", " 8.3 interval Estimation for the Comparison of Means from two Paired Samples "],
["8-4-two-sample-t-test-for-independent-samples-with-equal-variances.html", "8.4 two-Sample t test for independent Samples with Equal Variances", " 8.4 two-Sample t test for independent Samples with Equal Variances "],
["8-5-interval-estimation-for-the-comparison-of-means-from-two-independent-samples-equal-variance-case.html", "8.5 interval Estimation for the Comparison of Means from two independent Samples (Equal Variance Case)", " 8.5 interval Estimation for the Comparison of Means from two independent Samples (Equal Variance Case) "],
["8-6-testing-for-the-equality-of-two-variances.html", "8.6 testing for the Equality of two Variances", " 8.6 testing for the Equality of two Variances "],
["8-7-two-sample-t-test-for-independent-samples-with-unequal-variances.html", "8.7 two-Sample t test for independent Samples with Unequal Variances", " 8.7 two-Sample t test for independent Samples with Unequal Variances "],
["8-8-estimation-of-sample-size-and-power-for-comparing-two-means.html", "8.8 Estimation of Sample Size and Power for Comparing two Means", " 8.8 Estimation of Sample Size and Power for Comparing two Means "],
["8-9-the-treatment-of-outliers.html", "8.9 the treatment of outliers", " 8.9 the treatment of outliers "],
["8-10-derivation-of-equation-8-13.html", "8.10 derivation of Equation 8.13", " 8.10 derivation of Equation 8.13 "],
["9-nonparametric-methods.html", "9 Nonparametric Methods ", " 9 Nonparametric Methods "],
["9-1-introduction-5.html", "9.1 introduction", " 9.1 introduction "],
["9-2-the-sign-test.html", "9.2 the Sign test", " 9.2 the Sign test "],
["9-3-the-wilcoxon-signed-rank-test.html", "9.3 the Wilcoxon Signed-Rank test", " 9.3 the Wilcoxon Signed-Rank test "],
["9-4-the-wilcoxon-rank-sum-test.html", "9.4 the Wilcoxon Rank-Sum test", " 9.4 the Wilcoxon Rank-Sum test "],
["9-5-permutation-tests.html", "9.5 Permutation tests", " 9.5 Permutation tests "],
["10-hypothesis-testingcategoricaldata.html", "10 hypothesis testing:Categoricaldata ", " 10 hypothesis testing:Categoricaldata "],
["10-1-introduction-6.html", "10.1 introduction", " 10.1 introduction "],
["10-2-two-sample-test-for-binomial-proportions.html", "10.2 two-Sample test for Binomial Proportions", " 10.2 two-Sample test for Binomial Proportions "],
["10-3-fishers-exact-test.html", "10.3 Fisher’s Exact test", " 10.3 Fisher’s Exact test "],
["10-4-two-sample-test-for-binomial-proportions-for-matched-pair-data-mcnemars-test.html", "10.4 two-Sample test for Binomial Proportions for Matched-Pair data (Mcnemar’s test)", " 10.4 two-Sample test for Binomial Proportions for Matched-Pair data (Mcnemar’s test) "],
["10-5-estimation-of-sample-size-and-power-for-comparing-two-binomial-proportions.html", "10.5 Estimation of Sample Size and Power for Comparing two Binomial Proportions", " 10.5 Estimation of Sample Size and Power for Comparing two Binomial Proportions "],
["10-6-r-c-contingency-tables.html", "10.6 R × C Contingency tables", " 10.6 R × C Contingency tables "],
["10-7-chi-square-goodness-of-fit-test.html", "10.7 Chi-Square Goodness-of-Fit test", " 10.7 Chi-Square Goodness-of-Fit test "],
["10-8-the-kappa-statistic.html", "10.8 the Kappa Statistic", " 10.8 the Kappa Statistic "],
["10-9-derivation-of-selected-formulas.html", "10.9 derivation of Selected Formulas", " 10.9 derivation of Selected Formulas "],
["11-regression-and-correlation-methods.html", "11 Regression and Correlation Methods ", " 11 Regression and Correlation Methods "],
["11-1-introduction-7.html", "11.1 introduction", " 11.1 introduction "],
["11-2-general-concepts-2.html", "11.2 General Concepts", " 11.2 General Concepts "],
["11-3-fitting-regression-linesthe-method-of-least-squares.html", "11.3 Fitting Regression Lines—the Method of Least Squares", " 11.3 Fitting Regression Lines—the Method of Least Squares "],
["11-4-inferences-about-parameters-from-regression-lines.html", "11.4 inferences About Parameters from Regression Lines", " 11.4 inferences About Parameters from Regression Lines "],
["11-5-interval-estimation-for-linear-regression.html", "11.5 interval Estimation for Linear Regression", " 11.5 interval Estimation for Linear Regression "],
["11-6-assessing-the-goodness-of-fit-of-regression-lines.html", "11.6 Assessing the Goodness of Fit of Regression Lines", " 11.6 Assessing the Goodness of Fit of Regression Lines "],
["11-7-the-correlation-coefficient.html", "11.7 the Correlation Coefficient", " 11.7 the Correlation Coefficient "],
["11-8-statistical-inference-for-correlation-coefficients.html", "11.8 Statistical inference for Correlation Coefficients", " 11.8 Statistical inference for Correlation Coefficients "],
["11-9-multiple-regression.html", "11.9 Multiple Regression", " 11.9 Multiple Regression "],
["11-10-partial-and-multiple-correlation.html", "11.10 Partial and Multiple Correlation", " 11.10 Partial and Multiple Correlation "],
["11-11-rank-correlation.html", "11.11 Rank Correlation", " 11.11 Rank Correlation "],
["11-12-interval-estimation-for-rank-correlation-coefficients.html", "11.12 interval Estimation for Rank-Correlation Coefficients", " 11.12 interval Estimation for Rank-Correlation Coefficients "],
["11-13-derivation-of-equation-11-26.html", "11.13 derivation of Equation 11.26", " 11.13 derivation of Equation 11.26 "],
["12-multisample-inference.html", "12 Multisample inference ", " 12 Multisample inference "],
["12-1-introduction-to-the-one-way-analysis-of-variance.html", "12.1 introduction to the one-Way Analysis of Variance", " 12.1 introduction to the one-Way Analysis of Variance "],
["12-2-one-way-anovafixed-effects-model.html", "12.2 one-Way AnoVA—Fixed-Effects Model", " 12.2 one-Way AnoVA—Fixed-Effects Model "],
["12-3-hypothesis-testing-in-one-way-anova-fixed-effects-model.html", "12.3 hypothesis testing in one-Way AnoVA— Fixed-Effects Model", " 12.3 hypothesis testing in one-Way AnoVA— Fixed-Effects Model "],
["12-4-comparisons-of-specific-groups-in-one-way-anova.html", "12.4 Comparisons of Specific Groups in one- Way AnoVA", " 12.4 Comparisons of Specific Groups in one- Way AnoVA "],
["12-5-two-way-anova.html", "12.5 two-Way AnoVA", " 12.5 two-Way AnoVA "],
["12-6-the-kruskal-wallis-test.html", "12.6 the Kruskal-Wallis test", " 12.6 the Kruskal-Wallis test "],
["12-7-one-way-anovathe-random-effects-model.html", "12.7 one-Way AnoVA—the Random-Effects Model", " 12.7 one-Way AnoVA—the Random-Effects Model "],
["12-8-the-intraclass-correlation-coefficient.html", "12.8 the intraclass Correlation Coefficient", " 12.8 the intraclass Correlation Coefficient "],
["12-9-mixed-models.html", "12.9 Mixed Models", " 12.9 Mixed Models "],
["12-10-derivation-of-equation.html", "12.10 derivation of Equation", " 12.10 derivation of Equation "],
["13-design-and-analysis-techniques-for-epidemiologic-studies.html", "13 design and Analysis techniques for Epidemiologic Studies ", " 13 design and Analysis techniques for Epidemiologic Studies "],
["13-1-introduction-8.html", "13.1 introduction", " 13.1 introduction "],
["13-2-study-design.html", "13.2 Study design", " 13.2 Study design "],
["13-3-measures-of-effect-for-categorical-data.html", "13.3 Measures of Effect for Categorical data", " 13.3 Measures of Effect for Categorical data "],
["13-4-attributable-risk.html", "13.4 Attributable Risk", " 13.4 Attributable Risk "],
["13-5-confounding-and-standardization.html", "13.5 Confounding and Standardization", " 13.5 Confounding and Standardization "],
["13-6-methods-of-inference-for-stratified-categorical-datathe-mantel-haenszel-test.html", "13.6 Methods of inference for Stratified Categorical data—the Mantel-haenszel test", " 13.6 Methods of inference for Stratified Categorical data—the Mantel-haenszel test "],
["13-7-multiple-logistic-regression.html", "13.7 Multiple Logistic Regression", " 13.7 Multiple Logistic Regression "],
["13-8-extensions-to-logistic-regression.html", "13.8 Extensions to Logistic Regression", " 13.8 Extensions to Logistic Regression "],
["13-9-sample-size-estimation-for-logistic-regression.html", "13.9 Sample Size Estimation for Logistic Regression", " 13.9 Sample Size Estimation for Logistic Regression "],
["13-10-meta-analysis.html", "13.10 Meta-Analysis", " 13.10 Meta-Analysis "],
["13-11-equivalence-studies.html", "13.11 Equivalence Studies", " 13.11 Equivalence Studies "],
["13-12-the-cross-over-design.html", "13.12 the Cross-over design", " 13.12 the Cross-over design "],
["13-13-clustered-binary-data.html", "13.13 Clustered Binary data", " 13.13 Clustered Binary data "],
["13-14-longitudinal-data-analysis.html", "13.14 Longitudinal data Analysis", " 13.14 Longitudinal data Analysis "],
["13-15-measurement-error-methods.html", "13.15 Measurement-Error Methods", " 13.15 Measurement-Error Methods "],
["13-16-missing-data.html", "13.16 Missing data", " 13.16 Missing data "],
["13-17-derivation-of-100-1-ci-for-the-risk-difference.html", "13.17 derivation of 100% × (1 – α) Ci for the Risk difference", " 13.17 derivation of 100% × (1 – α) Ci for the Risk difference "],
["14-hypothesis-testing-person-time-data.html", "14 Hypothesis testing: Person-time data ", " 14 Hypothesis testing: Person-time data "],
["14-1-measure-of-effect-for-person-time-data.html", "14.1 Measure of Effect for Person-time data", " 14.1 Measure of Effect for Person-time data "],
["14-2-one-sample-inference-for-incidence-rate-data.html", "14.2 one-Sample inference for incidence-Rate data", " 14.2 one-Sample inference for incidence-Rate data "],
["14-3-two-sample-inference-for-incidence-rate-data.html", "14.3 two-Sample inference for incidence-Rate data", " 14.3 two-Sample inference for incidence-Rate data "],
["14-4-power-and-sample-size-estimation-for-person-time-data.html", "14.4 Power and Sample-Size Estimation for Person-time data", " 14.4 Power and Sample-Size Estimation for Person-time data "],
["14-5-inference-for-stratified-person-time-data.html", "14.5 Inference for Stratified Person-Time Data", " 14.5 Inference for Stratified Person-Time Data "],
["14-6-power-and-sample-size-estimation-for-stratified-person-time-data.html", "14.6 Power and Sample-Size Estimation for Stratified Person-Time Data", " 14.6 Power and Sample-Size Estimation for Stratified Person-Time Data "],
["14-7-testing-for-trend-incidence-rate-data.html", "14.7 Testing for Trend: Incidence-Rate Data", " 14.7 Testing for Trend: Incidence-Rate Data "],
["14-8-introduction-to-survival-analysis.html", "14.8 Introduction to Survival Analysis", " 14.8 Introduction to Survival Analysis "],
["14-9-estimation-of-survival-curves-the-kaplan-meier-estimator.html", "14.9 Estimation of Survival Curves: The Kaplan-Meier Estimator", " 14.9 Estimation of Survival Curves: The Kaplan-Meier Estimator "],
["14-10-the-log-rank-test.html", "14.10 The Log-Rank Test", " 14.10 The Log-Rank Test "],
["14-11-the-proportional-hazards-model.html", "14.11 The Proportional-Hazards Model", " 14.11 The Proportional-Hazards Model "],
["14-12-power-and-sample-size-estimation-under-the-proportional-hazards-model.html", "14.12 Power and Sample-Size Estimation under the Proportional-Hazards Model", " 14.12 Power and Sample-Size Estimation under the Proportional-Hazards Model "],
["14-13-parametric-survival-analysis.html", "14.13 Parametric Survival Analysis", " 14.13 Parametric Survival Analysis "],
["14-14-parametric-regression-models-for-survival-data.html", "14.14 Parametric Regression Models for Survival Data", " 14.14 Parametric Regression Models for Survival Data "],
["14-15-derivation-of-selected-formulas-1.html", "14.15 Derivation of Selected Formulas", " 14.15 Derivation of Selected Formulas "]
]
